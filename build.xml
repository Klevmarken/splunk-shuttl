<project xmlns:ivy="antlib:org.apache.ivy.ant" name="HadoopConnector" default="build" basedir=".">
	<!-- property files -->
	<property file="build.properties" />
	<property file="default.properties" />

	<property environment="env"/>
	<property name="user" value="${env.USER}" />

	<!-- project filesystem properties -->
	<property name="contribdir" value="${basedir}/contrib" />
	<property name="hadoop.dir" value="${contribdir}/hadoop-0.20.205.0" />
	<property name="flume.home" value="${contribdir}/flume-0.9.3-CDH3B4" />
	<property name="flumelib.home" value="${flume.home}/lib" />
	<property name="builddir" value="${basedir}/build" />
	<property name="build-cache" value="${basedir}/build-cache" />
	<property name="classdir" value="${builddir}/classes" />
	<property name="name" value="splunk-hadoop-connector" />
	<property name="testng.basename" value="testng-6.1.1" />
	<property name="testng.zip" value="${testng.basename}.zip" />
	<property name="testng.jar" value="${testng.basename}.jar" />
	<property name="jar.file" value="${builddir}/jar/${final.name}.jar" />
	<property name="hadoop.unittest.jar.file" value="${builddir}/jar/splunk_hadoop_unittests.jar" />
	<property name="appname" value="shep" />
	<property name="package.dir" value="${basedir}/package" />
	<property name="stage.dir" value="${builddir}/${appname}" />
	<property name="splunk-app.tar" value="${stage.dir}.tgz" />
	<property name="srcdir" value="${basedir}/src/java/" />
	<property name="testdir" value="${basedir}/test/java/" />
	<property name="libdir" value="${basedir}/lib" />
	<property name="ant-libs" value="${contribdir}/ant-libs" />
	<property name="splunk-sdk" value="${contribdir}/splunk-sdk" />

	<!-- hadoop properties -->
	<property name="hadoop.utils" value="${contribdir}/hadoop-utils" />
	<property name="hadoop.apache.org.hadoop.common.host" value="ftp://apache.cs.utah.edu/apache.org/hadoop/common" />
	<property name="hadoop.download.name.extracted" value="hadoop" />
	<property name="hadoop.download.name" value="hadoop.tar.gz" />
	<property name="hadoop.download.dir" value="${contribdir}" />
	<property name="hadoop.download.path" value="${hadoop.download.dir}/${hadoop.download.name}" />
	<property name="hadoop.extracted.dir" value="${build-cache}" />
	<property name="hadoop.extracted.path" value="${hadoop.extracted.dir}/${hadoop.download.name.extracted}" />
	<property name="hadoop.extracted.conf.dir" value="${hadoop.extracted.path}/conf" />

	<!-- hive properties -->
	<!--
	<property name="hive.download.url.prefix" value="http://www.gtlib.gatech.edu/pub/apache" />
	-->
	<property name="hive.download.url.prefix" value="http://mirror.cc.columbia.edu/pub/software/apache" />
	<property name="hive.basename" value="hive" />
	<property name="hive.versname" value="${hive.basename}-${hive.version}" />
	<property name="hive.fullname" value="${hive.versname}.tar.gz" />
	<property name="hive.download.name" value="${hive.fullname}" />
	<property name="hive.download.dir" value="${contribdir}" />
	<property name="hive.download.path" value="${hive.download.dir}/${hive.download.name}" />
	<property name="hive.extracted.dir" value="${build-cache}" />
	<property name="hive.extracted.path" value="${hive.extracted.dir}/${hive.versname}" />
	<property name="hive.home" value="${hive.extracted.path}" /> <!-- NOTE: unlike hadoop, will ignore env.HIVE_HOME -->
	<property name="hive.extracted.conf.dir" value="${hive.extracted.path}/conf" />
	<property name="hive.download.url" value="${hive.download.url.prefix}/${hive.basename}/${hive.versname}/${hive.fullname}" />
	<property name="hive.home" value="${hive.extracted.path}" />

	<!-- splunk properties -->
	<property name="splunk.tgz.dir" value="${basedir}/put-splunk-tgz-here" />
	<property name="splunk.extracted.dir" value="${build-cache}/splunk" />

	<!-- ivy properties -->
	<property name="ivy.utils" value="${contribdir}/ivy-utils" />
	<property name="ivy.xml.original" value="${ivy.utils}/ivy.xml" />
	<property name="ivy.xml.configurable" value="${basedir}/ivy.xml" />

	<path id="build.classpath">
		<pathelement location="${splunk-sdk}/splunk.jar" />
		<pathelement location="${flume.home}/flume-0.9.3-CDH3B4-core.jar" />
		<pathelement location="${basedir}/src/java/com/splunk/shep/connector/conf" />
		<pathelement path="${classdir}" />
		<fileset dir="${libdir}">
			<include name="*.jar" />
		</fileset>
	</path>

	<path id="ant.libs.classpath">
		<fileset dir="${ant-libs}">
			<include name="*.jar" />
		</fileset>
	</path>

	<taskdef resource="net/sf/antcontrib/antcontrib.properties" classpathref="ant.libs.classpath" />
	<taskdef resource="org/apache/ivy/ant/antlib.xml" uri="antlib:org.apache.ivy.ant" classpathref="ant.libs.classpath" />

	<!-- Cleans everything -->
	<target name="clean-all">
		<antcall>
			<target name="clean" />
			<target name="clean-downloads" />
		</antcall>
	</target>

	<!-- Clean build and build-cache dirs, and recreate them -->
	<target name="clean">
		<delete dir="${builddir}" />
		<mkdir dir="${builddir}" />
		<mkdir dir="${classdir}" />

		<!-- should not create things in the clean target? -->
		<antcall target="clean-build-cache" />
	</target>
	
	<target name="delete-built-classes">
		<delete dir="${classdir}" />
		<mkdir dir="${classdir}" />
	</target>

	<target name="clean-build-cache">
		<antcall>
			<target name="hadoop-teardown" />
			<target name="splunk-teardown" />
		</antcall>
		<delete dir="${build-cache}" />
		<mkdir dir="${build-cache}" />
	</target>

	<target name="clean-downloads">
		<antcall target="clean-hadoop-download" />
		<antcall target="clean-hive-download" />
		<antcall target="clean-ivy-downloads" />
	</target>

	<target name="clean-hadoop-download">
		<delete file="${hadoop.download.path}" />
	</target>

	<target name="clean-hive-download">
		<delete file="${hive.download.path}" />
	</target>

	<target name="clean-ivy-downloads">
		<delete>
			<fileset dir="${libdir}">
				<include name="*.jar" />
			</fileset>
		</delete>
	</target>

	<target name="clean-ivy-cache">
		<ivy:cleancache />
	</target>

	<!-- create a version.properties file from a template which is read by Version.java -->
	<target name="version">
		<exec executable="hostname" osfamily="unix" failifexecutionfails="false" outputproperty="hostname">
			<arg value="-s" />
		</exec>

		<exec executable="git" outputproperty="git.revision">
			<arg value="describe" />
			<arg value="--match" />
			<arg value="build" />
			<redirector>
				<outputfilterchain>
					<tokenfilter>
						<replaceregex pattern="^[^-]+-" replace="" />
					</tokenfilter>
				</outputfilterchain>
			</redirector>
		</exec>

		<property name="git.branch" value="fixmeWithTheRealBranchName" />

		<tstamp>
			<format property="time" timezone="America/Los_Angeles" pattern="MM/dd/yyyy hh:mm:ss aa" />
		</tstamp>
		<tstamp>
			<format property="noslashtime" timezone="America/Los_Angeles" pattern="yyyyMMddhhmmss" />
		</tstamp>
		<copy file="${srcdir}/com/splunk/shep/server/version.properties" todir="${classdir}/com/splunk/shep/server/" />
		<replace file="${classdir}/com/splunk/shep/server/version.properties">
			<replacefilter token="@DATE@" value="${time} US/Pacific" />
			<replacefilter token="@REVISION@" value="${git.revision}" />
			<replacefilter token="@BRANCH@" value="${git.branch}" />
			<replacefilter token="@BUILDMACHINE@" value="${hostname}" />
		</replace>

		<loadproperties>
			<file file="${classdir}/com/splunk/shep/server/version.properties" />
		</loadproperties>

		<!-- TODO add ${git.branch} into the suffix after properly fixing the retrival of branch name -->
		<property name="buildVersionSuffix" value="${version}-${git.revision}-${hostname}-${noslashtime}" />
		<echo message="Build version suffix: ${buildVersionSuffix}" />

	</target>

	<!-- compile all code, may in the future want to split out test and source builds -->
	<target name="compile" depends="ivy-resolve, do-compile" />
	<target name="do-compile" >
		<javac srcdir="${srcdir};${testdir}" destdir="${classdir}" includeAntRuntime="false" debug="true">
			<classpath refid="build.classpath" />
		</javac>
	</target>

	<target name="ivy-resolve" description="--> retrieve dependencies with ivy" depends="setup-ivy-xml" unless="isIvyResolved">
		<ivy:retrieve />
		<property name="isIvyResolved" value="true" />
	</target>

	<target name="setup-ivy-xml">
		<delete file="${ivy.xml.configurable}" />
		<copy file="${ivy.xml.original}" tofile="${ivy.xml.configurable}" />
		<replace file="${ivy.xml.configurable}">
			<replacefilter token="@HADOOP.VERSION@" value="${hadoop.version}" />
			<replacefilter token="@HIVE.VERSION@" value="${hive.version}" />
			<replacefilter token="@EDIT.THIS.FILE.DESCRIPTION.PATTERN@" value="DO NOT edit this file. This file was configued at runtime by build.xml. To see changes here, edit ${ivy.xml.original}" />
		</replace>
		<chmod perm="ugo-w" file="${ivy.xml.configurable}" />
	</target>

	<!-- run test via testNG -->
	<taskdef name="testng" classname="org.testng.TestNGAntTask" classpathref="ant.libs.classpath" />
	<target name="test" depends="compile">
		<antcall target="fast-tests" />
		<antcall target="slow-tests" />
	</target>

	<target name="test-all" depends="compile">
		<antcall target="test" />
		<antcall target="super-slow-tests" />
		<antcall target="hadoop-test" />
	</target>

	<propertyset id="testng">
		<propertyref name="splunk.username" />
		<propertyref name="splunk.password" />
		<propertyref name="splunk.home" />
		<propertyref name="splunk.host" />
		<propertyref name="splunk.mgmtport" />
		<propertyref name="hadoop.host" />
		<propertyref name="hadoop.port" />
	</propertyset>

	<target name="fast-tests" depends="compile, do-fast-tests" />
	<target name="do-fast-tests">
		<!-- Test group 'fast'. We think about adding more groups in the future -->
		<testng failureproperty="testng.failed" classpathref="build.classpath" groups="fast" outputdir="${builddir}/test-results">
			<propertyset refid="testng" />
			<classfileset dir="${classdir}" includes="**/*.class" />
		</testng>
	</target>

	<target name="slow-tests" depends="jar">
		<antcall>
			<target name="splunk-setup" />
		</antcall>
		<antcall>
			<target name="slow-TestNG-tests" />
		</antcall>
		<antcall>
			<target name="splunk-teardown" />
		</antcall>
	</target>

	<target name="slow-TestNG-tests" depends="compile,set-splunk-home">
		<!-- Test group 'fast'. We think about adding more groups in the future -->
		<testng classpathref="build.classpath" groups="slow" outputdir="${builddir}/test-results">
			<propertyset refid="testng" />
			<classfileset dir="${classdir}" includes="**/*.class" />
		</testng>
	</target>

	<!-- super-slow tests are tests that are in need of hadoop. -->
	<target name="super-slow-tests" depends="jar">
		<antcall>
			<target name="hadoop-setup" />
			<target name="hive-setup" />
			<target name="splunk-setup" />
		</antcall>
		<antcall>
			<target name="super-slow-TestNG-tests" />
			<target name="shell-tests" />
		</antcall>
		<antcall>
			<target name="splunk-teardown" />
			<target name="hadoop-teardown" />
			<target name="shep-teardown" />
		</antcall>
	</target>

	<target name="super-slow-TestNG-tests" depends="compile,set-splunk-home">
		<!-- Test group 'super-slow'. For tests that use hadoop -->
		<testng classpathref="build.classpath" groups="super-slow" outputdir="${builddir}/test-results">
			<propertyset refid="testng" />
			<classfileset dir="${classdir}" includes="**/*.class" />
		</testng>
	</target>

	<target name="shell-tests" depends="compile,set-splunk-home,set-hadoop-home">
		<echo>Running shell-tests..</echo>
		<exec dir="." executable="${basedir}/test/unittests/run-unittests.sh" osfamily="unix">
			<arg value="${hadoop.home}" />
			<arg value="${basedir}" />
			<arg value="${splunk.home}" />
			<arg value="${splunk.username}" />
			<arg value="${splunk.password}" />
		</exec>
	</target>

	<!-- Setting environment specific properties -->
	<target name="set-environment-property">
		<property environment="env" />
	</target>

	<target name="set-splunk-home" depends="set-user-env-splunk-home" unless="splunk.home">
		<property name="splunk.home" value="${splunk.extracted.dir}" />
	</target>

	<target name="set-user-env-splunk-home" depends="set-environment-property" if="defined.means.running.on.self.defined.splunk.home">
		<property name="splunk.home" value="${env.SPLUNK_HOME}" />
		<echo>You specified that you've got your own Splunk.</echo>
		<echo>Running shell tests with SPLUNK_HOME: ${splunk.home}</echo>
	</target>

	<target name="set-hadoop-home" depends="set-user-env-hadoop-home" unless="hadoop.home">
		<property name="hadoop.home" value="${hadoop.extracted.path}" />
	</target>

	<target name="set-user-env-hadoop-home" depends="set-environment-property" if="defined.means.running.on.self.defined.hadoop.home">
		<property name="hadoop.home" value="${env.HADOOP_HOME}" />
		<echo>You specified that you've got your own Hadoop.</echo>
		<echo>Running shell tests with HADOOP_HOME: ${hadoop.home}</echo>
	</target>

	<!-- Hive -->

	<target name="hive-download" depends="checkIfHiveIsDownloaded" unless="hive.isDownloaded">
		<echo>Downloading hive from ${hive.download.url}</echo>
		<exec executable="curl" osfamily="unix">
			<arg value="-s" />
			<arg value="${hive.download.url}" />
			<arg line="-o ${hive.download.name}" />
		</exec>
		<move file="${hive.download.name}" todir="${hive.download.dir}" />
	</target>

	<!-- extract to build cache-->
	<target name="hive-extract" depends="hive-download">
		<echo>Deleting ${hive.extracted.dir}/hive* ...</echo>
		<delete dir="${hive.extracted.dir}/hive*" />
		<echo>Extracting into ${hive.extracted.dir} ...</echo>
		<exec executable="sh" osfamily="unix">
			<arg value="-c" />
			<arg line="'tar xf ${hive.download.path} -C ${hive.extracted.dir}'" />
		</exec>
	</target>

	<target name="checkIfHiveIsDownloaded">
		<available file="${hive.download.path}" type="file" property="hive.isDownloaded"/>
		<echo>hive.isDownloaded is ${hive.isDownloaded}</echo>
		<echo>hive.download.path is ${hive.download.path}</echo>
	</target>
	
	<!-- Hive -->
	<target name="hive-setup" depends="set-hadoop-home">
		<!-- create the dirs in hdfs; hive keys off of hadoop.home  -->
		<echo>hadoop.home is ${hadoop.home} </echo>
		<exec executable="${hadoop.home}/bin/hadoop" osfamily="unix">
			<arg value="fs" />
			<arg value="-mkdir" />
			<arg value="/tmp" />
		</exec>
		<exec executable="${hadoop.home}/bin/hadoop" osfamily="unix">
			<arg value="fs" />
			<arg value="-mkdir" />
			<arg value="/user/hive/warehouse" />
		</exec>	
		<exec executable="${hadoop.home}/bin/hadoop" osfamily="unix">
			<arg value="fs" />
			<arg value="-chmod" />
			<arg value="g+w" />
			<arg value="/tmp" />
		</exec>
		<exec executable="${hadoop.home}/bin/hadoop" osfamily="unix">
			<arg value="fs" />
			<arg value="-chmod" />
			<arg value="g+w" />
			<arg value="/user/hive/warehouse" />
		</exec>
	</target>

	<!-- Start hive server; use screen to run in background -->
	<target name="hive-start" depends="set-hadoop-home">
		<!-- we'll use screen, and hive-$USER will be the name of the session -->
		<echo>hive.home=${hive.home} hive.server.port=${hive.server.port} screenvalue = ${hive.basename}-${user}
		</echo>
		<exec executable="screen" dir="${hive.home}" osfamily="unix">
			<env key="HADOOP_HOME" value="${hadoop.home}"/>
			<arg value="-S" />
			<arg value="${hive.basename}-${env.USER}" />
			<arg value="-d" />
			<arg value="-m" />
			<arg value="./bin/hive" />
			<arg value="--service" />
			<arg value="hiveserver" />
			<arg value="-p" />
			<arg value="${hive.server.port}" />
		</exec>
	</target>

	<!-- Stop hive server; quiting screen session may leave hive processes behind, kill those too -->
	<target name="hive-stop" depends="set-hadoop-home">
		<exec executable="screen" dir="${hive.home}" osfamily="unix">
			<env key="HADOOP_HOME" value="${hadoop.home}"/>
			<arg value="-S" />
			<arg value="${hive.basename}-${env.USER}" />
			<arg value="-X" />
			<arg value="quit" />
		</exec>
		<exec executable="bin/hiv3-kill.sh" osfamily="unix"/>
	</target>
 
	<!-- Hadoop -->
	<target name="hadoop-setup" depends="set-hadoop-home">
		<antcall>
			<target name="hadoop-prepare-build-cache" />
			<target name="hadoop-boot" />
		</antcall>
	</target>

	<target name="hadoop-prepare-build-cache" unless="defined.means.running.on.self.defined.hadoop.home" depends="set-hadoop-home">
		<antcall target="do-hadoop-prepare-build-cache" />
	</target>

	<target name="do-hadoop-prepare-build-cache" depends="checkHadoopInCache" unless="isHadoopInCache">
		<antcall>
			<target name="hadoop-download" />
			<target name="hadoop-extract" />
			<target name="hive-extract" />
			<target name="hadoop-copy-confs" />
			<target name="hadoop-format-namenode-unsafe" />
		</antcall>
	</target>

	<target name="checkHadoopInCache">
		<condition property="isHadoopInCache">
			<available file="${hadoop.extracted.path}" type="dir" />
		</condition>
	</target>

	<target name="hadoop-download" depends="checkIfHadoopNotDownloaded" if="isNotDownloaded">
		<antcall>
			<target name="do-hadoop-download" />
		</antcall>
	</target>

	<target name="do-hadoop-download" depends="set-hadoop-download-url">
		<echo>Downloading hadoop from ${hadoop.download.url}</echo>
		<exec executable="curl" osfamily="unix">
			<arg value="-s" />
			<arg value="${hadoop.download.url}" />
			<arg line="-o ${hadoop.download.name}" />
		</exec>
		<!-- this move will make hadoop.download.path make sence -->
		<move file="${hadoop.download.name}" todir="${hadoop.download.dir}" />
	</target>

	<target name="checkIfHadoopNotDownloaded">
		<!--PLEASEREVIEW:  brain hurts -->
		<condition property="isNotDownloaded">
			<not>
				<available file="${hadoop.download.path}" type="file" />
			</not>
		</condition>
	</target>

	<target name="set-hadoop-download-url">
		<if>
			<equals arg1="${hadoop.version}" arg2="0.20.203.0" />
			<then>
				<property name="hadoop.version.url" value="hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz" />
			</then>
		</if>
		<if>
			<equals arg1="${hadoop.version}" arg2="0.20.205.0" />
			<then>
				<property name="hadoop.version.url" value="hadoop-0.20.205.0/hadoop-0.20.205.0.tar.gz" />
			</then>
		</if>
		<if>
			<equals arg1="${hadoop.version}" arg2="1.0.0" />
			<then>
				<property name="hadoop.version.url" value="hadoop-1.0.0/hadoop-1.0.0.tar.gz" />
			</then>
		</if>
		<property name="hadoop.download.url" value="${hadoop.apache.org.hadoop.common.host}/${hadoop.version.url}" />
	</target>

	<!-- extract to build cache-->
	<target name="hadoop-extract" depends="checkIfHadoopIsDownloaded" unless="isDownloaded">
		<!-- PLEASEREVIEW: unclear if the check for downloaded is correct; better not to check
		and fail, no? -->
		<echo>Extracting...</echo>
		<exec executable="sh" osfamily="unix">
			<arg value="-c" />
			<arg line="'tar xf ${hadoop.download.path} -C ${hadoop.extracted.dir}'" />
		</exec>
		<exec executable="sh" osfamily="unix">
			<arg value="-c" />
			<arg line="'mv ${hadoop.extracted.dir}/hadoop-* ${hadoop.extracted.path}'" />
		</exec>
	</target>

	<target name="checkIfHadoopIsDownloaded">
		<!-- PLEASEREVIEW: didn't I see this target before? oh, no, I saw its evil twin -->
		<condition property="isDownloaded">
			<not>
				<available file="${hadoop.download.path}" type="file" />
			</not>
		</condition>
	</target>

	<target name="hadoop-copy-confs">
		<property name="hadoop.conf.dir" value="${hadoop.utils}/conf" />
		<copy todir="${hadoop.extracted.conf.dir}">
			<fileset dir="${hadoop.conf.dir}" includes="*" />
		</copy>
		<antcall target="hadoop-setup-copied-confs" />
	</target>

	<target name="hadoop-setup-copied-confs" depends="set-extracted-hadoop-conf-properties">
		<antcall>
			<target name="hadoop-conf-hadoop-env-sh" />
			<target name="hadoop-conf-core-site-xml" />
			<target name="hadoop-conf-mapred-site-xml" />
		</antcall>
	</target>

	<target name="set-extracted-hadoop-conf-properties">
		<property name="hadoop.env.sh" value="${hadoop.extracted.conf.dir}/hadoop-env.sh" />
		<property name="core-site.xml" value="${hadoop.extracted.conf.dir}/core-site.xml" />
		<property name="mapred-site.xml" value="${hadoop.extracted.conf.dir}/mapred-site.xml" />
	</target>

	<target name="hadoop-conf-hadoop-env-sh" depends="set-environment-property,set-hadoop-classpath">
		<fail unless="env.JAVA_HOME" message="You need to have JAVA_HOME set in order to get hadoop configured." />
		<replace file="${hadoop.env.sh}">
			<replacefilter token="@JAVA.HOME@" value="${env.JAVA_HOME}" />
			<replacefilter token="@HADOOP.PID.DIR@" value="${hadoop.extracted.path}/tmp" />
			<replacefilter token="@HADOOP.CLASSPATH@" value="$HADOOP_CLASSPATH:${hadoop.classpath}" />
		</replace>
	</target>

	<target name="set-hadoop-classpath" depends="jar">
		<property name="hadoop.classpath" value="${name.versioned.path}:${libdir}/commons-codec-1.4.jar:${libdir}/httpclient-4.1.2.jar:${libdir}/httpcore-4.1.2.jar:${splunk-sdk}/splunk.jar" />
	</target>

	<target name="hadoop-conf-core-site-xml">
		<replace file="${core-site.xml}">
			<replacefilter token="@HADOOP.TMP.DIR@" value="${hadoop.extracted.path}/tmp" />
			<replacefilter token="@HADOOP.HOST@" value="${hadoop.host}" />
			<replacefilter token="@HADOOP.PORT@" value="${hadoop.port}" />
		</replace>
	</target>

	<target name="hadoop-conf-mapred-site-xml">
		<replace file="${mapred-site.xml}">
			<replacefilter token="@HADOOP.HOST@" value="${hadoop.host}" />
			<replacefilter token="@HADOOP.JOBTRACKER.PORT@" value="${hadoop.jobtracker.port}" />
		</replace>
	</target>

	<!-- Stop, copy and start -->
	<target name="hadoop-boot">
		<antcall>
			<target name="hadoop-start" />
			<target name="hadoop-unsafe-mode" />
		</antcall>
	</target>

	<target name="hadoop-stop">
		<exec executable="${hadoop.home}/bin/stop-all.sh" osfamily="unix" />
	</target>

	<target name="hadoop-format-namenode-unsafe">
		<exec executable="sh" osfamily="unix">
			<arg value="-c" />
			<arg line="'${hadoop.home}/bin/hadoop namenode -format'" />
		</exec>
	</target>

	<target name="hadoop-start">
		<exec executable="${hadoop.home}/bin/start-all.sh" osfamily="unix" />
	</target>

	<target name="hadoop-unsafe-mode">
		<exec executable="${hadoop.home}/bin/hadoop" osfamily="unix">
			<arg line="dfsadmin -safemode leave" />
		</exec>
	</target>

	<target name="hadoop-teardown" depends="set-hadoop-home,checkForHadoopBinaries" if="existsHadoopBinaries">
		<antcall>
			<target name="hadoop-stop" />
		</antcall>
	</target>
	
	<target name="shep-teardown">
		<exec executable="curl">
			<arg value="-s"/>
			<arg line="http://localhost:9090/shep/rest/server/shutdown" />
		</exec>
	</target>

	<target name="checkForHadoopBinaries">
		<available file="${hadoop.home}/bin" type="dir" property="existsHadoopBinaries" />
	</target>

	<!-- Splunk -->
	<target name="splunk-setup" depends="set-splunk-home">
		<antcall>
			<target name="splunk-unpack-if-not-extracted" />
			<target name="splunk-configure-and-start" />
		</antcall>
	</target>

	<target name="splunk-configure-and-start">
		<antcall>
			<target name="install-shep-splunk-app" />
			<target name="splunk-start" />
			<target name="enable-shep-splunk-app" />
		</antcall>
	</target>

	<target name="splunk-unpack-if-not-extracted" unless="defined.means.running.on.self.defined.splunk.home">
		<antcall target="do-splunk-unpack-if-not-extracted" />
	</target>

	<target name="do-splunk-unpack-if-not-extracted" depends="checkForSplunkExtraction" if="isSplunkNotExtracted">
		<antcall target="splunk-unpack" />
	</target>

	<target name="checkForSplunkExtraction">
		<condition property="isSplunkNotExtracted">
			<not>
				<available file="${splunk.extracted.dir}/bin" type="dir" />
			</not>
		</condition>
	</target>

	<target name="splunk-unpack" depends="isSplunkTgzAvailable" if="foundSplunkTgz">
		<exec executable="sh" osfamily="unix">
			<arg value="-c" />
			<arg line="'tar xf ${splunk.tgz.dir}/*.tgz -C ${build-cache}'" />
		</exec>
	</target>

	<target name="isSplunkTgzAvailable">
		<pathconvert property="foundSplunkTgz" setonempty="false" pathsep=" ">
			<path>
				<fileset dir="${splunk.tgz.dir}" includes="splunk-*.tgz" />
			</path>
		</pathconvert>
		<fail unless="foundSplunkTgz" message="There has to be a splunk version packaged with .tgz in ${splunk.tgz.dir}" />
	</target>

	<target name="install-shep-splunk-app" depends="checkIfAlreadyInstalledShepApp" if="isNotInstalledShep">
		<antcall target="do-install-shep-splunk-app" />
	</target>

	<target name="set-shep-app-home" depends="set-splunk-home">
		<property name="splunk.apps.home" value="${splunk.home}/etc/apps" />
		<property name="shep.app.home" value="${splunk.apps.home}/shep" />
	</target>

	<target name="checkIfAlreadyInstalledShepApp" depends="set-shep-app-home">
		<condition property="isNotInstalledShep">
			<not>
				<available file="${shep.app.home}" type="dir" />
			</not>
		</condition>
	</target>

	<target name="do-install-shep-splunk-app" depends="create-splunk-app">
		<antcall target="splunk-stop" />
		<exec executable="tar" osfamily="unix">
			<arg value="xf" />
			<arg value="${splunk-app.tar}" />
			<arg line="-C ${splunk.apps.home}" />
		</exec>
	</target>

	<target name="re-install-shep-splunk-app" depends="set-splunk-home">
		<antcall>
			<target name="splunk-stop" />
			<target name="remove-shep-splunk-app" />
			<target name="splunk-configure-and-start" />
		</antcall>
	</target>

	<target name="remove-shep-splunk-app" depends="set-shep-app-home">
		<delete dir="${shep.app.home}" />
	</target>

	<target name="splunk-start">
		<exec executable="${splunk.home}/bin/splunk">
			<arg line="start --accept-license --no-prompt --answer-yes splunkd" />
		</exec>
	</target>

	<target name="splunk-start-splunkweb" depends="set-splunk-home">
		<exec executable="${splunk.home}/bin/splunk">
			<arg line="start splunkweb" />
		</exec>
	</target>

	<target name="splunk-stop">
		<exec executable="${splunk.home}/bin/splunk">
			<arg line="stop -f" />
		</exec>
	</target>

	<target name="splunk-restart">
		<exec executable="${splunk.home}/bin/splunk">
			<arg line="restart" />
		</exec>
	</target>

	<target name="enable-shep-splunk-app" depends="checkIfShepIsEnabled" unless="isShepAlreadyEnabled">
		<exec executable="curl" osfamily="unix">
			<arg value="-s" />
			<arg line="-u ${splunk.username}:${splunk.password}" />
			<arg line="-k https://localhost:8089/services/apps/local/shep/enable" />
			<arg line="-X POST" />
		</exec>
	</target>

	<target name="checkIfShepIsEnabled">
		<shellscript shell="sh" outputproperty="isEnabled">
			${splunk.home}/bin/splunk display app 2> /dev/null | grep shep | grep -o ENABLED
		</shellscript>
		<if>
			<equals arg1="${isEnabled}" arg2="ENABLED" />
			<then>
				<property name="isShepAlreadyEnabled" value="true" />
			</then>
		</if>
	</target>

	<target name="splunk-teardown" depends="set-splunk-home,checkForSplunkBinary" if="existsSplunkBinary">
		<antcall>
			<target name="splunk-stop" />
			<target name="splunk-clean-all" />
		</antcall>
	</target>

	<target name="checkForSplunkBinary">
		<available file="${splunk.home}/bin/splunk" type="file" property="existsSplunkBinary" />
	</target>

	<target name="splunk-clean-all">
		<exec executable="${splunk.home}/bin/splunk">
			<arg line="clean all -f" />
		</exec>
	</target>

	<target name="versionized-jar-name" depends="version">
		<loadproperties>
			<file file="${classdir}/com/splunk/shep/server/version.properties" />
		</loadproperties>
		<property name="name.versioned" value="${name}-${version}" />
		<property name="name.versioned.path" value="${builddir}/jar/${name.versioned}.jar" />
	</target>

	<!-- create the jar file -->
	<target name="jar" depends="compile,versionized-jar-name">
		<mkdir dir="${builddir}/jar" />
		<jar destfile="${name.versioned.path}" basedir="${builddir}/classes">
			<exclude name="com/splunk/shep/mapreduce/lib/rest/tests/**" />
			<exclude name="edu/jhu/nlp/**" />
			<manifest>
				<attribute name="Main-Class" value="com.splunk.shep.server.ShepJettyServer" />
			</manifest>
		</jar>
		<jar destfile="${hadoop.unittest.jar.file}" basedir="${builddir}/classes">
			<include name="com/splunk/shep/mapreduce/lib/rest/tests/**" />
			<include name="edu/jhu/nlp/**" />
		</jar>
	</target>

	<!-- build ready for testing -->
	<target name="build">
		<antcall>
			<target name="clean" />
			<target name="compile" />
			<target name="jar" />
		</antcall>
	</target>

	<!-- create the ditribution package, which assume will be a superset of the jar, and we'll tgz it at the end -->
	<target name="dist" depends="clean,compile,jar">
		<antcall target="create-splunk-app" />
		<copy tofile="${stage.dir}-${buildVersionSuffix}.tgz" file="${splunk-app.tar}" />
	</target>

	<target name="create-splunk-app" depends="jar">
		<antcall>
			<target name="stage-splunk-app" />
			<target name="tar-splunk-app" />
		</antcall>
	</target>

	<target name="stage-splunk-app">
		<!-- make the staging directory -->
		<mkdir dir="${stage.dir}" />
		<!-- copy all the package files into there -->
		<copy todir="${stage.dir}">
			<fileset dir="${package.dir}" excludes="lib/*" />
		</copy>
		<antcall target="copy-libs-to-stage-dir" />

		<!-- update the version in the app.conf file -->
		<replace file="${stage.dir}/default/app.conf">
			<replacefilter token="@VERSION@" value="${version}" />
		</replace>
		<!-- copy the jar in there -->
		<copy file="${name.versioned.path}" todir="${stage.dir}/bin" />
	</target>

	<target name="tar-splunk-app">
		<tar destfile="${splunk-app.tar}" compression="gzip">
			<tarfileset dir="${stage.dir}/bin" prefix="${appname}/bin" filemode="755">
				<exclude name="README" />
			</tarfileset>
			<tarfileset dir="${stage.dir}/bin" prefix="${appname}/bin">
				<include name="README" />
			</tarfileset>
			<tarfileset dir="${stage.dir}" prefix="${appname}">
				<exclude name="bin/**" />
			</tarfileset>
		</tar>
	</target>

	<target name="copy-libs-to-stage-dir">
		<antcall>
			<target name="copy-lib-jars-to-stage-dir" />
		</antcall>
	</target>

	<target name="copy-lib-jars-to-stage-dir">
		<copy todir="${stage.dir}/lib">
			<fileset dir="${libdir}" includes="*.jar" excludes="*-sources.jar,*-javadoc.jar" />
		</copy>
	</target>

	<!-- run the connector, this needs to be fixed -->
	<target name="run">
		<java jar="build/jar/${final.name}.jar" fork="true" />
	</target>

	<target name="splunk2flume2console">
		<java classname="com.splunk.shep.connector.tests.Splunk2Flume2ConsoleTest">
			<arg value="src/java/com/splunk/shep/connector/tests/splunk2flume.conf" />
			<classpath refid="build.classpath" />
		</java>
	</target>

	<target name="stateMachineTest">
		<java classname="com.splunk.shep.connector.tests.StateMachineTest">
			<arg value="src/java/com/splunk/shep/connector/tests/s2s.data" />
			<classpath refid="build.classpath" />
		</java>
	</target>
	
	<!-- Run by hooks -->
	<target name="pre-commit" depends="delete-built-classes, do-compile, do-fast-tests" >
		<fail if="testng.failed" message="Fast tests have failed."/>
	</target>
	
	<!--hadoop unit test -->
	<property name="report.dir"  value="${builddir}/junitreport"/>
	<target name="hadoop-test" depends="compile">
		<mkdir dir="${report.dir}"/>
        <junit printsummary="yes">
            <classpath>
                <path refid="build.classpath"/>
            </classpath>
        	<formatter type="xml"/>
            <batchtest fork="yes" todir="${report.dir}">
                <fileset dir="${testdir}" includes="**/Test*.java"/>
            </batchtest>
        </junit>
        <junitreport todir="${report.dir}">
            <fileset dir="${report.dir}" includes="TEST-*.xml"/>
            <report todir="${report.dir}"/>
        </junitreport>
    </target>
</project>
