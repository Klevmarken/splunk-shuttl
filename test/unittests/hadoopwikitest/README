This test loads Wikipedia pages into HDFS. The summary is loaded into Splunk via a MapReduce job (runtest.sh). 
One of the fields in the Summary is a list of links to other pages in Wikipedia from this page. The second part of this test runs a MapReduce job to read this field and calculates the number of references to a page from across all the pages of Wikipedia.

An example for loading wikipedia data into hdfs is given below -
./wiki2hdfs.sh http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p000000010p000010000.bz2 hdfs://localhost:54310/wordcount/wikitest/wiki2.seq

The load job is run - 
./runtest.sh <id>
where the id is used to create a unique ouptut directory in Hadoop

The WikiLinkCount job is run -
./runtest.sh <id> <splunk-host> <splunk-userid> <splunk-passwd>
